import pandas as pd
import numpy as np
import os
import joblib
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from train_scoring_model import CreditScoringModel

class RejectInference:
    """
    Reject Inference í´ë˜ìŠ¤
    
    Methods:
    1. Hard Cutoff: í™•ë¥  ê¸°ì¤€ìœ¼ë¡œ Good/Bad í• ë‹¹
    2. Fuzzy Augmentation: í™•ë¥ ì„ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
    3. Parceling: ê° ì ìˆ˜ êµ¬ê°„ë³„ Bad Rate ì ìš©
    """
    
    def __init__(self, approved_df, rejected_df, feature_cols, target_col='target'):
        self.approved_df = approved_df.copy()
        self.rejected_df = rejected_df.copy()
        self.feature_cols = feature_cols
        self.target_col = target_col
        
        # ìŠ¹ì¸ ë°ì´í„°ë¡œ ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ (Check train_scoring_model.py for class definition)
        self.base_model = CreditScoringModel(feature_cols)
        self.base_model.fit(approved_df, approved_df[target_col])
        
        # ê±°ì ˆ ê³ ê° í™•ë¥  ì˜ˆì¸¡
        self.rejected_df['pred_prob'] = self.base_model.predict_proba(rejected_df)
        self.rejected_df['pred_score'] = self.base_model.predict_score(rejected_df)
        
    def hard_cutoff(self, cutoff=0.5):
        """
        Method 1: Hard Cutoff
        - í™•ë¥ ì´ cutoff ì´ìƒì´ë©´ Good, ë¯¸ë§Œì´ë©´ Bad
        - ê°€ì¥ ê°„ë‹¨í•˜ì§€ë§Œ ì •ë³´ ì†ì‹¤ ìˆìŒ
        """
        output = f"\n{'='*60}\n"
        output += f"ğŸ”¹ Hard Cutoff Method (cutoff={cutoff})\n"
        output += f"{'='*60}\n"
        
        rejected_copy = self.rejected_df.copy()
        rejected_copy[self.target_col] = (rejected_copy['pred_prob'] >= cutoff).astype(int)
        
        # ê²°ê³¼ í†µê³„
        n_good = (rejected_copy[self.target_col] == 1).sum()
        n_bad = (rejected_copy[self.target_col] == 0).sum()
        
        output += f"ê±°ì ˆ ê³ ê° ì¤‘ ì¶”ì • Good: {n_good:,} ({n_good/len(rejected_copy)*100:.1f}%)\n"
        output += f"ê±°ì ˆ ê³ ê° ì¤‘ ì¶”ì • Bad:  {n_bad:,} ({n_bad/len(rejected_copy)*100:.1f}%)\n"
        
        # í†µí•© ë°ì´í„°
        combined = pd.concat([self.approved_df, rejected_copy], ignore_index=True)
        
        return combined, rejected_copy, output
    
    def fuzzy_augmentation(self, scaling_factor=0.8):
        """
        Method 2: Fuzzy Augmentation
        - ê° ê±°ì ˆ ê³ ê°ì„ Goodê³¼ Badë¡œ ë³µì œ
        - ì˜ˆì¸¡ í™•ë¥ ì„ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©
        - scaling_factor: ê±°ì ˆ ê³ ê°ì˜ Bad Rate ì¡°ì • (< 1ì´ë©´ ë” ë‚˜ìœ ê²ƒìœ¼ë¡œ ê°„ì£¼)
        """
        output = f"\n{'='*60}\n"
        output += f"ğŸ”¹ Fuzzy Augmentation Method (scaling={scaling_factor})\n"
        output += f"{'='*60}\n"
        
        rejected_copy = self.rejected_df.copy()
        
        # í™•ë¥  ì¡°ì • (ê±°ì ˆ ê³ ê°ì€ ì¼ë°˜ì ìœ¼ë¡œ ë” ë‚˜ì¨)
        adjusted_prob = rejected_copy['pred_prob'] * scaling_factor
        
        # Good ë ˆì½”ë“œ ìƒì„± (ê°€ì¤‘ì¹˜ = ì¡°ì •ëœ í™•ë¥ )
        good_records = rejected_copy.copy()
        good_records[self.target_col] = 1
        good_records['weight'] = adjusted_prob
        
        # Bad ë ˆì½”ë“œ ìƒì„± (ê°€ì¤‘ì¹˜ = 1 - ì¡°ì •ëœ í™•ë¥ )
        bad_records = rejected_copy.copy()
        bad_records[self.target_col] = 0
        bad_records['weight'] = 1 - adjusted_prob
        
        # ìŠ¹ì¸ ë°ì´í„°ì— ê°€ì¤‘ì¹˜ ì¶”ê°€
        approved_weighted = self.approved_df.copy()
        approved_weighted['weight'] = 1.0
        
        # í†µí•©
        combined = pd.concat([approved_weighted, good_records, bad_records], ignore_index=True)
        
        # í†µê³„
        effective_good = good_records['weight'].sum()
        effective_bad = bad_records['weight'].sum()
        
        output += f"ê±°ì ˆ ê³ ê° Effective Good: {effective_good:,.1f}\n"
        output += f"ê±°ì ˆ ê³ ê° Effective Bad:  {effective_bad:,.1f}\n"
        output += f"Effective Bad Rate: {effective_bad/(effective_good+effective_bad)*100:.1f}%\n"
        
        return combined, output
    
    def parceling(self, n_bins=10):
        """
        Method 3: Parceling
        - ì ìˆ˜ êµ¬ê°„ë³„ë¡œ ìŠ¹ì¸ ê³ ê°ì˜ Bad Rate ê³„ì‚°
        - ê±°ì ˆ ê³ ê°ì—ê²Œ í•´ë‹¹ êµ¬ê°„ì˜ Bad Rate í™•ë¥ ë¡œ Good/Bad í• ë‹¹
        """
        np.random.seed(42)  # For reproducibility
        
        output = f"\n{'='*60}\n"
        output += f"ğŸ”¹ Parceling Method (bins={n_bins})\n"
        output += f"{'='*60}\n"
        
        # ìŠ¹ì¸ ê³ ê° ì ìˆ˜ ê³„ì‚°
        approved_copy = self.approved_df.copy()
        approved_copy['score'] = self.base_model.predict_score(approved_copy)
        
        # ì ìˆ˜ êµ¬ê°„ ìƒì„±
        score_min = min(approved_copy['score'].min(), self.rejected_df['pred_score'].min())
        score_max = max(approved_copy['score'].max(), self.rejected_df['pred_score'].max())
        bins = np.linspace(score_min - 1, score_max + 1, n_bins + 1)
        
        approved_copy['score_bin'] = pd.cut(approved_copy['score'], bins=bins, labels=False)
        
        # êµ¬ê°„ë³„ Bad Rate ê³„ì‚°
        bin_stats = approved_copy.groupby('score_bin').agg({
            self.target_col: ['count', 'mean']
        }).reset_index()
        bin_stats.columns = ['score_bin', 'count', 'good_rate']
        bin_stats['bad_rate'] = 1 - bin_stats['good_rate']
        
        output += "\nğŸ“Š Score Bin Statistics:\n"
        output += bin_stats.to_string(index=False) + "\n"
        
        # ê±°ì ˆ ê³ ê°ì— ì ìš©
        rejected_copy = self.rejected_df.copy()
        rejected_copy['score_bin'] = pd.cut(rejected_copy['pred_score'], bins=bins, labels=False)
        
        # ê° êµ¬ê°„ì˜ Bad Rateë¥¼ í™•ë¥ ë¡œ ì‚¬ìš©í•˜ì—¬ Good/Bad í• ë‹¹
        def assign_target(row):
            bin_idx = row['score_bin']
            if pd.isna(bin_idx) or bin_idx not in bin_stats['score_bin'].values:
                # êµ¬ê°„ ë°–ì´ë©´ ì „ì²´ Bad Rate ì‚¬ìš©
                bad_rate = approved_copy[self.target_col].mean()
            else:
                bad_rate = bin_stats.loc[bin_stats['score_bin'] == bin_idx, 'bad_rate'].values[0]
            
            # Bad Rateë§Œí¼ì˜ í™•ë¥ ë¡œ Bad(0) í• ë‹¹
            return 0 if np.random.random() < bad_rate else 1
        
        rejected_copy[self.target_col] = rejected_copy.apply(assign_target, axis=1)
        
        # ê²°ê³¼ í†µê³„
        n_good = (rejected_copy[self.target_col] == 1).sum()
        n_bad = (rejected_copy[self.target_col] == 0).sum()
        
        output += f"\nê±°ì ˆ ê³ ê° í• ë‹¹ ê²°ê³¼:\n"
        output += f"  Good: {n_good:,} ({n_good/len(rejected_copy)*100:.1f}%)\n"
        output += f"  Bad:  {n_bad:,} ({n_bad/len(rejected_copy)*100:.1f}%)\n"
        
        # í†µí•©
        combined = pd.concat([
            self.approved_df, 
            rejected_copy.drop(columns=['pred_prob', 'pred_score', 'score_bin'])
        ], ignore_index=True)
        
        return combined, rejected_copy, output

if __name__ == "__main__":
    # Load data
    base_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    data_path = os.path.join(base_path, 'data', 'raw', 'telecom_data.csv')
    df = pd.read_csv(data_path)
    
    approved_df = df[df['status'] == 'approved'].copy()
    rejected_df = df[df['status'] == 'rejected'].copy()
    feature_cols = ['age', 'income', 'credit_history_months', 
                    'num_credit_accounts', 'debt_ratio', 'num_late_payments']
    
    # Run Inference
    ri = RejectInference(approved_df, rejected_df, feature_cols)

    # 1. Hard Cutoff
    combined_hard, rejected_hard, out_hard = ri.hard_cutoff(cutoff=0.5)
    
    # 2. Fuzzy Augmentation
    combined_fuzzy, out_fuzzy = ri.fuzzy_augmentation(scaling_factor=0.7)
    
    # 3. Parceling
    combined_parcel, rejected_parcel, out_parcel = ri.parceling(n_bins=10)
    
    # Print and Save
    full_report = "\n" + "=" * 80 + "\n"
    full_report += "ğŸ”„ REJECT INFERENCE COMPARISON\n"
    full_report += "=" * 80 + "\n"
    full_report += out_hard
    full_report += out_fuzzy
    full_report += out_parcel
    
    print(full_report)
    
    with open(os.path.join(base_path, 'reports', 'reject_inference_methods.txt'), 'w', encoding='utf-8') as f:
        f.write(full_report)
